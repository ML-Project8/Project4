{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Four: Extending Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Austin Chen, Luke Hansen, Oscar Vallner*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's world, it might be easy to make the assumption that those who have higher paying jobs must have achieved a higher level of education at some point in their lifetimes. For example, one might expect an employee who achieved a Masters degree to get paid more than an employee who only achieved a high school diploma. For the last couple decades, it has been a topic of wide cultural debate, whether higher level degrees of education (Masters and Doctoral) are worthy investments in careers.\n",
    "\n",
    "Therefore, we have chosen to analyze a subset of 40 year's worth of the United States' federal payroll records. This large dataset contains the payroll records of government employees for the past 40 years. Our federal payroll data was obtained through the Freedom of Information Act by BuzzFeed news. We have chosen government payroll data for a few different reasons. First, having such a large volume of data (40 years worth of payroll records) provides us with flexibility; with a large dataset, we have the ability to subsample--whether it be randomized, by department, or year. Second, as patrons and benefactors of the United States Government, we have chosen to place faith in the assumption that officially published government data is accurate. The dataset contains several attributes regarding education level, payment, government agency division, etc. with each specific employees name abstracted. By trusting the integrity of this data, we can potentially create a useful, real-world classifier to predict an employee's highest education level. Even if the government has lied about the accuracy of the data, rendering the classifier useless for their internal operations, the classifier can still be useful to the public, contingent on an high accuracy rate.\n",
    "\n",
    "\n",
    "While there is no singular use-case that this data is meant to solve, we have decided to pick apart this payroll data in order to see if there are any meaningful conclusions we can draw on any government worker's career and life decisions, based on data of their current job position. Perhaps it is _indeed_ true that those who achieve higher levels of education end up with higher compensations. It could be the case, however, that higher government compensation is merely a function of a longer length of service. We hope that through building a classifier, we can answer some of these questions.\n",
    "\n",
    "Due to the sheer volume of the dataset, we have decided to take a subset of the 40 years of data, and narrow our focus to an easy-to-grasp classification problem regarding educational level. Given attributes such as the agency division, age, length of service, pay, and more, we will be attempting to clasisfy the highest level of education each employee received.\n",
    "\n",
    "There may not be any __short-term__, immediate actions one could take with these results. However, through time, a trained Logistic Regression classifier could aid the United States government in analyzing the value of educational degrees in government jobs. By looking at factors such as pay, length of service, and age, the United States government could more appropriately create compensation models for their employees based off their highest level of education. Or, they could use any conclusions drawn from the classifer to verify whether the compensation consistency between many employees with the same degree. Furthermore, a classification model that is able to accurately predict an employee's education level could potentially be extended to contexts beyond government jobs. With enough data exploration, a classification model such as ours could be experimented with in other paradigms of the career market. _Perhaps_ a Masters degree isn't reflective of higher salaries in a government job, but could be indicative of higher compensation in a discipline such as engineering, business, or education. \n",
    "\n",
    "We ultimately decided to divide our classification problem into four classes. Each class represents the HIGHEST edcuation level that an employee attained.\n",
    "\n",
    "1. Highschool (or under) not completed\n",
    "2. Highschool Completed\n",
    "3. Bachelor's Completed\n",
    "4. Graduate degree completed (Masters and up)\n",
    "\n",
    "As a baseline, our classification model should at least beat random, 25%. However, because this classification model could directly affect the United States government and its decisions, we should plan for our model to be as accurate as possible. In order to make helpful, informed decisions for the United States government based on our classification model, we wish to obtain at least 85% accuracy.\n",
    "\n",
    "\n",
    "---\n",
    "Link to dataset: https://ia600608.us.archive.org/16/items/opm-federal-employment-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import plotly\n",
    "from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis, Bar, Line\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Class Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/2014-clean.csv', encoding = 'latin1',low_memory=False)\n",
    "df_adjust = df\n",
    "\n",
    "# Encodes education into a much more manageable prediction attribute\n",
    "df_adjust.Education[df_adjust.Education < 4] = 1\n",
    "df_adjust.Education[(df_adjust.Education > 3) & (df_adjust.Education < 13)] = 2\n",
    "df_adjust.Education[(df_adjust.Education > 12) & (df_adjust.Education < 17)] = 3\n",
    "df_adjust.Education[df_adjust.Education > 16] = 4\n",
    "\n",
    "#Binary encode, 8 is not a Supervisor, <8 are unordered types of Supervisors\n",
    "df_adjust.SupervisoryStatus[df_adjust.SupervisoryStatus < 8] = 1\n",
    "df_adjust.SupervisoryStatus[df_adjust.SupervisoryStatus == 8] = 0\n",
    "\n",
    "#ensure data is integer encoded\n",
    "df_adjust = df_adjust[np.isfinite(df_adjust['Education'])]\n",
    "df_adjust = df_adjust[np.isfinite(df_adjust['SupervisoryStatus'])]\n",
    "df_adjust.Education = df_adjust.Education.astype(int)\n",
    "df_adjust.SupervisoryStatus = df_adjust.SupervisoryStatus.astype(int)\n",
    "\n",
    "# Encodes Length of Service (LOS)\n",
    "df_adjust.LOS[df_adjust.LOS == '< 1'] = 0\n",
    "df_adjust.LOS[df_adjust.LOS == '1-2'] = 1\n",
    "df_adjust.LOS[df_adjust.LOS == '3-4'] = 2\n",
    "df_adjust.LOS[df_adjust.LOS == '5-9'] = 3\n",
    "df_adjust.LOS[df_adjust.LOS == '10-14'] = 4\n",
    "df_adjust.LOS[df_adjust.LOS == '15-19'] = 5\n",
    "df_adjust.LOS[df_adjust.LOS == '20-24'] = 6\n",
    "df_adjust.LOS[df_adjust.LOS == '25-29'] = 7\n",
    "df_adjust.LOS[df_adjust.LOS == '30-34'] = 8\n",
    "df_adjust.LOS[df_adjust.LOS == '35+'] = 9\n",
    "df_adjust.LOS[df_adjust.LOS == 'UNSP'] = np.NaN\n",
    "\n",
    "#we have more than ehough data. Data with missing elements is considered unreliable.\n",
    "df_adjust = df_adjust.dropna()\n",
    "\n",
    "#convert tointeger\n",
    "df_adjust.Pay = df_adjust.Pay.astype(int)\n",
    "df_adjust.LOS = df_adjust.LOS.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    306080\n",
      "3    216420\n",
      "4    137024\n",
      "1      8342\n",
      "Name: Education, dtype: int64\n",
      "667866\n"
     ]
    }
   ],
   "source": [
    "print(df_adjust.Education.value_counts())\n",
    "print(len(df_adjust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VETERANS HEALTH ADMINISTRATION', 'INTERNAL REVENUE SERVICE', 'FEDERAL AVIATION ADMINISTRATION', 'NATIONAL INSTITUTES OF HEALTH', 'VETERANS BENEFITS ADMINISTRATION', 'NATIONAL PARK SERVICE', 'FOOD AND DRUG ADMINISTRATION', 'OFC SEC HEALTH AND HUMAN SERVICES', 'ENVIRONMENTAL PROTECTION AGENCY', 'FEDERAL EMERGENCY MANAGEMENT AGENCY', 'INDIAN HEALTH SERVICE', 'BUREAU OF PRISONS/FEDERAL PRISON SYSTEM', 'DEPARTMENT OF ENERGY']\n",
      "Length:  153661\n"
     ]
    }
   ],
   "source": [
    "# This block of code removes all agencies without at least 10,000 people in it\n",
    "\n",
    "agency_names = (df_adjust.AgencyName.value_counts() > 10000)\n",
    "trimmed_agencies = []\n",
    "\n",
    "for i in range(0, len(agency_names)):\n",
    "    if agency_names[i] == True:\n",
    "        trimmed_agencies.append(agency_names.axes[0][i])\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "print(trimmed_agencies)\n",
    "\n",
    "df_adjust = df_adjust[~df_adjust['AgencyName'].isin(trimmed_agencies)]\n",
    "print(\"Length: \", len(df_adjust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  Education PayPlan  LOS Category     Pay  SupervisoryStatus Schedule  \\\n",
      "0  35-39          3      GS    3        P  149993                  0        F   \n",
      "1  35-39          3      GS    3        P  149993                  0        F   \n",
      "2  40-44          3      GS    4        P  145504                  0        F   \n",
      "3  30-34          3      GS    3        P  124995                  0        F   \n",
      "4  30-34          3      GS    3        P  145827                  0        F   \n",
      "\n",
      "   NSFTP                     AgencyName  \n",
      "0      1  OFFICES, BOARDS AND DIVISIONS  \n",
      "1      1  OFFICES, BOARDS AND DIVISIONS  \n",
      "2      1  OFFICES, BOARDS AND DIVISIONS  \n",
      "3      1  OFFICES, BOARDS AND DIVISIONS  \n",
      "4      1  OFFICES, BOARDS AND DIVISIONS  \n",
      "           Education            LOS            Pay  SupervisoryStatus  \\\n",
      "count  153661.000000  153661.000000  153661.000000      153661.000000   \n",
      "mean        2.895354       4.259916   93296.271826           0.178399   \n",
      "std         0.789885       2.319148   34303.527020           0.382850   \n",
      "min         1.000000       0.000000    5000.000000           0.000000   \n",
      "25%         2.000000       3.000000   66008.000000           0.000000   \n",
      "50%         3.000000       4.000000   91650.000000           0.000000   \n",
      "75%         4.000000       6.000000  116901.000000           0.000000   \n",
      "max         4.000000       9.000000  261853.000000           1.000000   \n",
      "\n",
      "               NSFTP  \n",
      "count  153661.000000  \n",
      "mean        1.087042  \n",
      "std         0.281898  \n",
      "min         1.000000  \n",
      "25%         1.000000  \n",
      "50%         1.000000  \n",
      "75%         1.000000  \n",
      "max         2.000000  \n",
      "Age  :  13\n",
      "Education  :  4\n",
      "PayPlan  :  40\n",
      "LOS  :  10\n",
      "Category  :  6\n",
      "Pay  :  9225\n",
      "SupervisoryStatus  :  2\n",
      "Schedule  :  7\n",
      "NSFTP  :  2\n",
      "AgencyName  :  161\n"
     ]
    }
   ],
   "source": [
    "#df_usable = df_adjust[['Agency','Age','Education','PayPlan', 'LOS', 'Category','Pay', 'SupervisoryStatus', 'Schedule', 'NSFTP', 'AgencyName']]\n",
    "df_usable = df_adjust[['Age','Education','PayPlan', 'LOS', 'Category','Pay', 'SupervisoryStatus', 'Schedule', 'NSFTP', 'AgencyName']]\n",
    "\n",
    "print(df_usable.head())\n",
    "print(df_usable.describe())\n",
    "for x in df_usable:\n",
    "    print(x, \" : \", len(df_usable[x].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode necessary data\n",
    "df_dummies = pd.get_dummies(data=df_usable, columns=['Age', 'PayPlan', 'Category', 'SupervisoryStatus', 'Schedule', 'NSFTP', 'AgencyName'])\n",
    "\n",
    "df_dummies = df_dummies.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "#X_train, X_test, y_train1, y_test1 = train_test_split(df_dummies.drop('Education',1), df_dummies.Education, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train-Test-Split & Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-versus-all Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparing Results to sci-kit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integrating with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH WORK --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad,C):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "        return np.sum((y-yhat)**2)-C*np.sum(wnew**2)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParallelMultiClassLogisticRegression(MultiClassLogisticRegression):\n",
    "    @staticmethod\n",
    "    def par_logistic(yval,eta,iters,C,X,y):\n",
    "        y_binary = y==yval # create a binary problem\n",
    "        # train the binary classifier for this class\n",
    "        hblr = BFGSBinaryLogisticRegression(eta,iters,C)\n",
    "        hblr.fit(X,y_binary)\n",
    "        return hblr\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        backend = 'threading' #'multiprocessing'\n",
    "        \n",
    "        self.classifiers_ = Parallel(n_jobs=-1,backend=backend)(\n",
    "            delayed(self.par_logistic)(yval,self.eta,self.iters,self.C,X,y) for yval in self.unique_)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self, eta, iters=20, method=\"steepdescent\", regularization=\"none\", C=0.0):\n",
    "        self.eta = eta\n",
    "        self.iters = iters\n",
    "        self.method = method\n",
    "        self.regularization = regularization\n",
    "        self.C = C\n",
    "        \n",
    "    \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.00009334e-05   1.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  2.00009334e-05   1.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  2.74906532e-05   9.99999999e-01   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  6.34920633e-05   9.99999996e-01   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  3.53556779e-05   9.99999995e-01   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  2.18895018e-05   9.99999998e-01   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscar/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_base = normalize(df_dummies.drop('Education',1).as_matrix())\n",
    "y_base = df_dummies.Education\n",
    "h, w = X_base.shape\n",
    "print(X_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.64 s, sys: 682 ms, total: 6.33 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "%time X_transform = pca.fit_transform(X_base)\n",
    "pca_res = pca.components_.reshape((n_components,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_explained_variance(pca):\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "individual explained variance",
         "type": "bar",
         "y": [
          0.5645312795057132,
          0.0685549470108992,
          0.04934003960302757,
          0.02933942549214945,
          0.02466942437404782,
          0.019780410309073324,
          0.014389128351245074,
          0.013787553219563684,
          0.01314985284581585,
          0.012702106625303317,
          0.012142837668529907,
          0.011712667714938172,
          0.010946792422558376,
          0.010080328853302406,
          0.008651464289029461,
          0.008345201251068397,
          0.007879823239427883,
          0.007494438895095543,
          0.0068101838201684965,
          0.006218117674501224,
          0.0051698907813317275,
          0.004792479561357985,
          0.00440015514159081,
          0.0043275074448318905,
          0.00423417676731402,
          0.0038253027011810265,
          0.003598542420543079,
          0.0033449700071496668,
          0.0031759647427031554,
          0.002802160737534,
          0.002772147687988429,
          0.0024981490585948636,
          0.0024259462878243525,
          0.002351614879468915,
          0.0021660502228277047,
          0.0021115048404076867,
          0.002062563863923484,
          0.0020101164339996064,
          0.0018989443798327619,
          0.0018638904081471422,
          0.0014852205005202945,
          0.0013196441970971685,
          0.0012728182761705292,
          0.001116775369239577,
          0.0010138959723182501,
          0.0009905869431459508,
          0.0009239615885455602,
          0.0008776420791088792,
          0.0008189432752634362,
          0.0007858654788376571
         ]
        },
        {
         "name": "cumulative explained variance",
         "type": "scatter",
         "y": [
          0.5645312795057132,
          0.6330862265166124,
          0.68242626611964,
          0.7117656916117894,
          0.7364351159858372,
          0.7562155262949105,
          0.7706046546461556,
          0.7843922078657193,
          0.7975420607115351,
          0.8102441673368385,
          0.8223870050053683,
          0.8340996727203065,
          0.8450464651428649,
          0.8551267939961673,
          0.8637782582851967,
          0.8721234595362651,
          0.8800032827756931,
          0.8874977216707886,
          0.8943079054909571,
          0.9005260231654583,
          0.9056959139467899,
          0.9104883935081479,
          0.9148885486497387,
          0.9192160560945706,
          0.9234502328618845,
          0.9272755355630655,
          0.9308740779836085,
          0.9342190479907582,
          0.9373950127334614,
          0.9401971734709954,
          0.9429693211589838,
          0.9454674702175787,
          0.947893416505403,
          0.9502450313848719,
          0.9524110816076996,
          0.9545225864481073,
          0.9565851503120307,
          0.9585952667460304,
          0.9604942111258631,
          0.9623581015340102,
          0.9638433220345305,
          0.9651629662316277,
          0.9664357845077982,
          0.9675525598770378,
          0.9685664558493561,
          0.969557042792502,
          0.9704810043810476,
          0.9713586464601565,
          0.9721775897354199,
          0.9729634552142575
         ]
        }
       ],
       "layout": {
        "xaxis": {
         "title": "Principal components"
        },
        "yaxis": {
         "title": "Explained variance ratio"
        }
       }
      },
      "text/html": [
       "<div id=\"46399a71-bd4e-4b1d-8e15-ab7070a156f8\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"46399a71-bd4e-4b1d-8e15-ab7070a156f8\", [{\"type\": \"bar\", \"y\": [0.5645312795057132, 0.0685549470108992, 0.04934003960302757, 0.02933942549214945, 0.02466942437404782, 0.019780410309073324, 0.014389128351245074, 0.013787553219563684, 0.01314985284581585, 0.012702106625303317, 0.012142837668529907, 0.011712667714938172, 0.010946792422558376, 0.010080328853302406, 0.008651464289029461, 0.008345201251068397, 0.007879823239427883, 0.007494438895095543, 0.0068101838201684965, 0.006218117674501224, 0.0051698907813317275, 0.004792479561357985, 0.00440015514159081, 0.0043275074448318905, 0.00423417676731402, 0.0038253027011810265, 0.003598542420543079, 0.0033449700071496668, 0.0031759647427031554, 0.002802160737534, 0.002772147687988429, 0.0024981490585948636, 0.0024259462878243525, 0.002351614879468915, 0.0021660502228277047, 0.0021115048404076867, 0.002062563863923484, 0.0020101164339996064, 0.0018989443798327619, 0.0018638904081471422, 0.0014852205005202945, 0.0013196441970971685, 0.0012728182761705292, 0.001116775369239577, 0.0010138959723182501, 0.0009905869431459508, 0.0009239615885455602, 0.0008776420791088792, 0.0008189432752634362, 0.0007858654788376571], \"name\": \"individual explained variance\"}, {\"type\": \"scatter\", \"y\": [0.5645312795057132, 0.6330862265166124, 0.68242626611964, 0.7117656916117894, 0.7364351159858372, 0.7562155262949105, 0.7706046546461556, 0.7843922078657193, 0.7975420607115351, 0.8102441673368385, 0.8223870050053683, 0.8340996727203065, 0.8450464651428649, 0.8551267939961673, 0.8637782582851967, 0.8721234595362651, 0.8800032827756931, 0.8874977216707886, 0.8943079054909571, 0.9005260231654583, 0.9056959139467899, 0.9104883935081479, 0.9148885486497387, 0.9192160560945706, 0.9234502328618845, 0.9272755355630655, 0.9308740779836085, 0.9342190479907582, 0.9373950127334614, 0.9401971734709954, 0.9429693211589838, 0.9454674702175787, 0.947893416505403, 0.9502450313848719, 0.9524110816076996, 0.9545225864481073, 0.9565851503120307, 0.9585952667460304, 0.9604942111258631, 0.9623581015340102, 0.9638433220345305, 0.9651629662316277, 0.9664357845077982, 0.9675525598770378, 0.9685664558493561, 0.969557042792502, 0.9704810043810476, 0.9713586464601565, 0.9721775897354199, 0.9729634552142575], \"name\": \"cumulative explained variance\"}], {\"xaxis\": {\"title\": \"Principal components\"}, \"yaxis\": {\"title\": \"Explained variance ratio\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"46399a71-bd4e-4b1d-8e15-ab7070a156f8\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"46399a71-bd4e-4b1d-8e15-ab7070a156f8\", [{\"type\": \"bar\", \"y\": [0.5645312795057132, 0.0685549470108992, 0.04934003960302757, 0.02933942549214945, 0.02466942437404782, 0.019780410309073324, 0.014389128351245074, 0.013787553219563684, 0.01314985284581585, 0.012702106625303317, 0.012142837668529907, 0.011712667714938172, 0.010946792422558376, 0.010080328853302406, 0.008651464289029461, 0.008345201251068397, 0.007879823239427883, 0.007494438895095543, 0.0068101838201684965, 0.006218117674501224, 0.0051698907813317275, 0.004792479561357985, 0.00440015514159081, 0.0043275074448318905, 0.00423417676731402, 0.0038253027011810265, 0.003598542420543079, 0.0033449700071496668, 0.0031759647427031554, 0.002802160737534, 0.002772147687988429, 0.0024981490585948636, 0.0024259462878243525, 0.002351614879468915, 0.0021660502228277047, 0.0021115048404076867, 0.002062563863923484, 0.0020101164339996064, 0.0018989443798327619, 0.0018638904081471422, 0.0014852205005202945, 0.0013196441970971685, 0.0012728182761705292, 0.001116775369239577, 0.0010138959723182501, 0.0009905869431459508, 0.0009239615885455602, 0.0008776420791088792, 0.0008189432752634362, 0.0007858654788376571], \"name\": \"individual explained variance\"}, {\"type\": \"scatter\", \"y\": [0.5645312795057132, 0.6330862265166124, 0.68242626611964, 0.7117656916117894, 0.7364351159858372, 0.7562155262949105, 0.7706046546461556, 0.7843922078657193, 0.7975420607115351, 0.8102441673368385, 0.8223870050053683, 0.8340996727203065, 0.8450464651428649, 0.8551267939961673, 0.8637782582851967, 0.8721234595362651, 0.8800032827756931, 0.8874977216707886, 0.8943079054909571, 0.9005260231654583, 0.9056959139467899, 0.9104883935081479, 0.9148885486497387, 0.9192160560945706, 0.9234502328618845, 0.9272755355630655, 0.9308740779836085, 0.9342190479907582, 0.9373950127334614, 0.9401971734709954, 0.9429693211589838, 0.9454674702175787, 0.947893416505403, 0.9502450313848719, 0.9524110816076996, 0.9545225864481073, 0.9565851503120307, 0.9585952667460304, 0.9604942111258631, 0.9623581015340102, 0.9638433220345305, 0.9651629662316277, 0.9664357845077982, 0.9675525598770378, 0.9685664558493561, 0.969557042792502, 0.9704810043810476, 0.9713586464601565, 0.9721775897354199, 0.9729634552142575], \"name\": \"cumulative explained variance\"}], {\"xaxis\": {\"title\": \"Principal components\"}, \"yaxis\": {\"title\": \"Explained variance ratio\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "- TODO: One hot encoding took 10 components (8 which needed to be one hot encoded) and increased the dimensionality to 466 components. Normalization and PCA reduced this number back down to needing 31 components to explain 96% of the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "labels": [
          1,
          2,
          3,
          4
         ],
         "type": "pie",
         "values": [
          831,
          54324,
          58600,
          39906
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 300,
        "title": "Binary Class Distribution",
        "width": 500
       }
      },
      "text/html": [
       "<div id=\"5af10f8e-a8a1-42df-8daf-83d5d2778770\" style=\"height: 300px; width: 500px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5af10f8e-a8a1-42df-8daf-83d5d2778770\", [{\"labels\": [1, 2, 3, 4], \"values\": [831, 54324, 58600, 39906], \"type\": \"pie\"}], {\"title\": \"Binary Class Distribution\", \"autosize\": false, \"width\": 500, \"height\": 300}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"5af10f8e-a8a1-42df-8daf-83d5d2778770\" style=\"height: 300px; width: 500px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5af10f8e-a8a1-42df-8daf-83d5d2778770\", [{\"labels\": [1, 2, 3, 4], \"values\": [831, 54324, 58600, 39906], \"type\": \"pie\"}], {\"title\": \"Binary Class Distribution\", \"autosize\": false, \"width\": 500, \"height\": 300}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_graph = df_adjust.Education # make problem binary\n",
    "\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "graph1 = {'labels': np.unique(y_graph),\n",
    "          'values': np.bincount(y_graph)[1:],\n",
    "            'type': 'pie'}\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Binary Class Distribution',\n",
    "                'autosize':False,\n",
    "                'width':500,\n",
    "                'height':300}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y = y_base.values\n",
    "X = X_transform\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.350047180555\n",
      "confusion matrix\n",
      " [[    0   157     0     0]\n",
      " [    0 10758     0     0]\n",
      " [    0 11724     0     0]\n",
      " [    0  8094     0     0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.352422477467\n",
      "confusion matrix\n",
      " [[    0   175     0     0]\n",
      " [    0 10831     0     0]\n",
      " [    0 11756     0     0]\n",
      " [    0  7971     0     0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.349851950672\n",
      "confusion matrix\n",
      " [[    0   161     0     0]\n",
      " [    0 10752     0     0]\n",
      " [    0 11769     0     0]\n",
      " [    0  8051     0     0]]\n",
      "CPU times: user 1min 2s, sys: 7.84 s, total: 1min 10s\n",
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = RegularizedLogisticRegression(eta=0.1,iterations=100) # get object\n",
    "\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI KIT LEARN SHIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.379168971464\n",
      "confusion matrix\n",
      " [[    0     0   160     0]\n",
      " [    0     0 10952     0]\n",
      " [    0     0 11653     0]\n",
      " [    0     0  7968     0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.379526892916\n",
      "confusion matrix\n",
      " [[    0     0   176     0]\n",
      " [    0     0 10908     0]\n",
      " [    0     0 11664     0]\n",
      " [    0     0  7985     0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.386587707025\n",
      "confusion matrix\n",
      " [[    0     0   157     0]\n",
      " [    0     0 10755     0]\n",
      " [    0     0 11881     0]\n",
      " [    0     0  7940     0]]\n",
      "CPU times: user 4.48 s, sys: 226 ms, total: 4.71 s\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_sk.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_sk.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aite so basically, scikit learn is better than us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
